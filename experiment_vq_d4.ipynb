{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf53d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# CHANGE this for whatever GPU index you have\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5255d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tfkl = tf.keras.layers\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from aldi.modeling.layers import DownLevel, UpLevel\n",
    "from aldi.modeling.callbacks import ReconstructionPlotCallback, CodebookResetter\n",
    "from aldi.modeling.vq import Autoencoder, RVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e2d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "(train_images, _), (test_images, _) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_images = train_images.astype(np.float32) / 255.\n",
    "test_images = test_images.astype(np.float32) / 255.\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train_images).shuffle(50000).batch(batch_size, num_parallel_calls=tf.data.AUTOTUNE, drop_remainder=True)\n",
    "test_data = tf.data.Dataset.from_tensor_slices(test_images).batch(batch_size, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "train_data = train_data.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for ind, img in enumerate(test_images[:64]):\n",
    "    plt.subplot(8, 8, ind+1)\n",
    "    plt.imshow(img, cmap=\"Greys\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plt.hist(test_images.reshape(-1), bins=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_stack(inputs, filters, strides, blocks_per_level):\n",
    "    inputs = tfkl.Conv2D(filters[0], 3, padding=\"same\")(inputs)\n",
    "    for level_ind, (level_filters, level_strides) in enumerate(zip(filters[1:], strides)):\n",
    "        inputs = DownLevel(2,\n",
    "                           blocks_per_level,\n",
    "                           level_filters,\n",
    "                           level_filters,\n",
    "                           3,\n",
    "                           level_strides,\n",
    "                           normalization=tfkl.BatchNormalization,\n",
    "                           name=\"down_level\" + str(level_ind))(inputs)\n",
    "        \n",
    "    return inputs\n",
    "\n",
    "\n",
    "def decoder_stack(inputs, filters, strides, blocks_per_level):\n",
    "    inputs = tfkl.Conv2D(filters[0], 3, padding=\"same\")(inputs)\n",
    "    for level_ind, (level_filters, level_strides) in enumerate(zip(filters[1:], strides)):\n",
    "        inputs = UpLevel(2,\n",
    "                         blocks_per_level,\n",
    "                         level_filters,\n",
    "                         level_filters,\n",
    "                         3,\n",
    "                         level_strides,\n",
    "                         normalization=tfkl.BatchNormalization,\n",
    "                         name=\"up_level\" + str(level_ind))(inputs)\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "histories = []\n",
    "\n",
    "d = 4\n",
    "codebook_powers = range(1, 14)\n",
    "codebook_sizes = [2**power for power in codebook_powers]\n",
    "betas = [0.002, 0.02, 0.2, 2.]\n",
    "betas = [b/d for b in betas]\n",
    "for cbs in codebook_sizes:\n",
    "    models.append([])\n",
    "    histories.append([])\n",
    "    print(\"\\n\\n\\nRUNNING codebook size = {}\".format(cbs))\n",
    "    for beta in betas:\n",
    "        print(\"\\n\\n\\nRUNNING beta = {}\".format(beta))\n",
    "\n",
    "        inp = tf.keras.Input((32, 32, 3))\n",
    "\n",
    "        blocks_per_level = 2\n",
    "        filters = [16, 32, 64, 128]\n",
    "        strides = [2, 2, 2]\n",
    "\n",
    "        encoder_final = encoder_stack(inp, filters, strides, blocks_per_level)\n",
    "        encoder_final = tfkl.Conv2D(d, 1, padding=\"same\")(encoder_final)\n",
    "\n",
    "        encoder = tf.keras.Model(inp, encoder_final, name=\"encoder\")\n",
    "\n",
    "        decoder_input = tf.keras.Input(encoder_final.shape[1:])\n",
    "        decoder_output = decoder_stack(decoder_input, list(reversed(filters)), list(reversed(strides)), blocks_per_level)\n",
    "        decoder_final = tfkl.Conv2D(3, 1,  padding=\"same\")(decoder_output)\n",
    "\n",
    "        decoder = tf.keras.Model(decoder_input, decoder_final, name=\"decoder\")\n",
    "\n",
    "        quantizer = RVQ(cbs, encoder_final.shape[-1], 1)\n",
    "\n",
    "        model = Autoencoder(inp, encoder, decoder, tf.keras.losses.MeanSquaredError(), \n",
    "                            quantizer=quantizer, beta=beta, name=\"autoencoder\")\n",
    "        model.summary(expand_nested=True)\n",
    "        \n",
    "        \n",
    "        batches = []\n",
    "        ind = 0\n",
    "        for batch in train_data:\n",
    "            dummyenc = encoder(batch)\n",
    "            batches.append(dummyenc)\n",
    "            ind+=1\n",
    "            if ind >= 4:\n",
    "                break\n",
    "        dummyenc = tf.concat(batches, axis=0)\n",
    "        \n",
    "        quantizer.init_with_k_means(dummyenc, n_init=1, batch_n_multiplier=4)\n",
    "\n",
    "        \n",
    "        train_steps = 500000\n",
    "        n_data = 50000\n",
    "        n_epochs = train_steps // (n_data // batch_size)\n",
    "        optimizer = tf.optimizers.Adam()\n",
    "\n",
    "        model.compile(optimizer=optimizer, jit_compile=True)\n",
    "\n",
    "\n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1, factor=0.5,\n",
    "                                                         min_delta=0.000001)\n",
    "        earlystop = tf.keras.callbacks.EarlyStopping(patience=6, verbose=1, restore_best_weights=True,\n",
    "                                                         min_delta=0.000001)\n",
    "\n",
    "        average_code_use = batch_size*4*4 / cbs\n",
    "        history = model.fit(train_data, validation_data=test_data, epochs=n_epochs, \n",
    "                 callbacks=[ReconstructionPlotCallback(test_images[:16], 10, clip=True),\n",
    "                            CodebookResetter(1, threshold=average_code_use/256, iteration_source=train_data),\n",
    "                            reduce_lr, earlystop])\n",
    "\n",
    "        models[-1].append(model)\n",
    "        histories[-1].append(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = []\n",
    "d_losses = np.zeros((len(codebook_sizes), len(betas)))\n",
    "for cb_i, cb in enumerate(codebook_sizes):\n",
    "    for b_i, beta in enumerate(betas):\n",
    "        model = models[cb_i][b_i]\n",
    "        hmm = model.evaluate(test_data)\n",
    "        d_losses[cb_i, b_i] = hmm[0]\n",
    "    \n",
    "    best_models.append(models[cb_i][np.argmin(d_losses[cb_i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0710895",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(codebook_sizes, d_losses.min(axis=1), \"-*\")\n",
    "plt.xlabel(\"d\")\n",
    "plt.ylabel(\"Validation MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482275f",
   "metadata": {},
   "outputs": [],
   "source": [
    " d_losses.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c620e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"losses_d4.npy\", d_losses.min(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for history in histories:\n",
    "    for key in history:\n",
    "        vals = history[key]\n",
    "        plt.plot(vals)\n",
    "        plt.title(key)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7345e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for model in best_models:\n",
    "    encoded = model.encoder.predict(test_data)[:1000]\n",
    "    quantized, _, _ = model.quantizer(encoded)\n",
    "    quantized = quantized.numpy()\n",
    "\n",
    "    dotprod = (encoded * quantized).sum(axis=-1) / (np.linalg.norm(encoded,axis=-1)*np.linalg.norm(quantized,axis=-1))\n",
    "    errors.append((1-dotprod).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec6ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    encoded = model.encoder.predict(test_data)\n",
    "    plt.hist(encoded.reshape(-1), bins=250)\n",
    "    plt.show()\n",
    "    enc_flat = encoded.reshape((-1, d))\n",
    "    plt.scatter(enc_flat[:, 0], enc_flat[:, 1], marker=\".\", alpha=0.1)\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, model in zip(ds, models):\n",
    "    model.save(\"basic_d{}\".format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54671f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "for d, history in zip(ds, histories):\n",
    "    with open(\"basic_d{}.pkl\".format(d), \"wb\") as file:\n",
    "        pickle.dump(history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b147c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
